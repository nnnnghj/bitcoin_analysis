{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time range\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=730)  # About 2 years of data\n",
    "\n",
    "# Use yfinance to get Bitcoin and Ethereum price data\n",
    "btc_data = yf.download('BTC-USD', start=start_date, end=end_date)\n",
    "eth_data = yf.download('ETH-USD', start=start_date, end=end_date)\n",
    "\n",
    "# View the first few rows of Bitcoin data\n",
    "print(\"Bitcoin price data sample:\")\n",
    "btc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check what columns actually exist in the dataframe\n",
    "print(\"btc_data column names:\")\n",
    "print(btc_data.columns.tolist())\n",
    "\n",
    "# Also check if the dataframe is empty\n",
    "print(\"\\nbtc_data shape:\", btc_data.shape)\n",
    "print(\"\\nbtc_data first few rows:\")\n",
    "print(btc_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic information about the data\n",
    "print(\"Bitcoin price data information:\")\n",
    "btc_data.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing value check:\")\n",
    "print(btc_data.isnull().sum())\n",
    "\n",
    "# Basic statistical description\n",
    "print(\"\\nBasic statistical description:\")\n",
    "btc_data.describe()\n",
    "\n",
    "# If there is a 'Close' column\n",
    "btc_data['Returns'] = btc_data['Close'].pct_change() * 100\n",
    "eth_data['Returns'] = eth_data['Close'].pct_change() * 100\n",
    "\n",
    "# Display modified columns\n",
    "print(\"Columns after calculating returns:\", btc_data.columns.tolist())\n",
    "\n",
    "# Plot price time series\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(btc_data.index, btc_data['Close'], label='BTC Price')\n",
    "plt.title('Bitcoin Price Historical Trend', fontsize=15)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot return distribution\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.histplot(btc_data['Returns'].dropna(), kde=True)\n",
    "plt.title('Bitcoin Daily Returns Distribution', fontsize=15)\n",
    "plt.xlabel('Daily Returns (%)')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "btc_data['Returns'].dropna().plot(kind='line')\n",
    "plt.title('Bitcoin Daily Returns Time Series', fontsize=15)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Returns (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate volatility (20-day rolling standard deviation)\n",
    "btc_data['Volatility'] = btc_data['Returns'].rolling(window=20).std()\n",
    "\n",
    "# Plot volatility\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(btc_data.index, btc_data['Volatility'], color='red')\n",
    "plt.title('Bitcoin 20-Day Rolling Volatility', fontsize=15)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def explore_dataset(filepath, name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analyzing dataset: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load data using semicolon as separator\n",
    "    print(f\"\\nLoading {filepath}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=';', encoding='utf-8')\n",
    "        print(f\"Successfully loaded data using semicolon as delimiter.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load with semicolon delimiter: {e}\")\n",
    "        try:\n",
    "            # Try more flexible method\n",
    "            df = pd.read_csv(filepath, sep=';', encoding='utf-8', error_bad_lines=False, warn_bad_lines=True)\n",
    "            print(f\"Successfully loaded data (ignoring error lines).\")\n",
    "        except Exception as e2:\n",
    "            print(f\"All loading methods failed: {e2}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # 1. Basic data information\n",
    "    print(f\"\\n1. {name} basic information:\")\n",
    "    print(f\"   Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "    print(\"\\n   Data types and non-null counts:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # 2. Display first few rows\n",
    "    print(f\"\\n2. {name} data samples (first 5 rows):\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # 3. Column name analysis\n",
    "    print(f\"\\n3. {name} column analysis:\")\n",
    "    columns = df.columns.tolist()\n",
    "    print(f\"   Column names: {columns}\")\n",
    "    \n",
    "    # 4. Find potential key columns\n",
    "    # Look for sentiment-related columns\n",
    "    sentiment_cols = [col for col in columns if any(term in col.lower() for term in ['sentiment', 'emotion', 'score', 'compound', 'positive', 'negative'])]\n",
    "    print(f\"\\n4.1 Potential sentiment columns: {sentiment_cols if sentiment_cols else 'Not found'}\")\n",
    "    \n",
    "    # Look for date/time-related columns\n",
    "    date_cols = [col for col in columns if any(term in col.lower() for term in ['date', 'time', 'day', 'hour', 'timestamp'])]\n",
    "    print(f\"4.2 Potential time columns: {date_cols if date_cols else 'Not found'}\")\n",
    "    \n",
    "    # Look for price-related columns\n",
    "    price_cols = [col for col in columns if any(term in col.lower() for term in ['price', 'value', 'close', 'open', 'high', 'low'])]\n",
    "    print(f\"4.3 Potential price columns: {price_cols if price_cols else 'Not found'}\")\n",
    "    \n",
    "    # 5. Data distribution analysis\n",
    "    # Statistical analysis of numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    if numerical_cols:\n",
    "        print(f\"\\n5.1 Statistical analysis of numerical columns:\")\n",
    "        print(df[numerical_cols].describe())\n",
    "    \n",
    "    # 6. Visualization analysis\n",
    "    # If sentiment columns are found, plot their distribution\n",
    "    if sentiment_cols:\n",
    "        for col in sentiment_cols[:2]:  # Limit to analyzing only the first two sentiment columns\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            if df[col].dtype == 'object':  # Use count plot for categorical data\n",
    "                df[col].value_counts().head(10).plot(kind='bar')\n",
    "            else:  # Use histogram for numerical data\n",
    "                sns.histplot(df[col].dropna(), kde=True)\n",
    "            plt.title(f'{name} - {col} Distribution', fontsize=15)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # If both date and sentiment columns are found, plot time series\n",
    "    if date_cols and sentiment_cols:\n",
    "        try:\n",
    "            date_col = date_cols[0]\n",
    "            sentiment_col = sentiment_cols[0]\n",
    "            \n",
    "            # Try to convert date column\n",
    "            if df[date_col].dtype == 'object':\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "            \n",
    "            # Check if conversion was successful\n",
    "            if pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "                # Aggregate sentiment data by date\n",
    "                time_unit = 'D'  # Default to aggregate by day\n",
    "                \n",
    "                # If there are too many data points, consider aggregating by day\n",
    "                if df[date_col].nunique() > 100:\n",
    "                    df['date_truncated'] = df[date_col].dt.floor(time_unit)\n",
    "                    time_series = df.groupby('date_truncated')[sentiment_col].mean()\n",
    "                else:\n",
    "                    time_series = df.groupby(date_col)[sentiment_col].mean()\n",
    "                \n",
    "                plt.figure(figsize=(14, 7))\n",
    "                time_series.plot()\n",
    "                plt.title(f'{name} - {sentiment_col} Trend Over Time', fontsize=15)\n",
    "                plt.xlabel('Time')\n",
    "                plt.ylabel(sentiment_col)\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # If price data is also available, plot the relationship between price and sentiment\n",
    "                if price_cols:\n",
    "                    price_col = price_cols[0]\n",
    "                    \n",
    "                    # Create price data aggregated by date\n",
    "                    if 'date_truncated' in df.columns:\n",
    "                        price_series = df.groupby('date_truncated')[price_col].mean()\n",
    "                    else:\n",
    "                        price_series = df.groupby(date_col)[price_col].mean()\n",
    "                    \n",
    "                    # Create dual Y-axis chart\n",
    "                    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "                    \n",
    "                    color = 'tab:blue'\n",
    "                    ax1.set_xlabel('Time')\n",
    "                    ax1.set_ylabel(sentiment_col, color=color)\n",
    "                    ax1.plot(time_series.index, time_series.values, color=color)\n",
    "                    ax1.tick_params(axis='y', labelcolor=color)\n",
    "                    \n",
    "                    ax2 = ax1.twinx()\n",
    "                    color = 'tab:red'\n",
    "                    ax2.set_ylabel(price_col, color=color)\n",
    "                    ax2.plot(price_series.index, price_series.values, color=color)\n",
    "                    ax2.tick_params(axis='y', labelcolor=color)\n",
    "                    \n",
    "                    fig.tight_layout()\n",
    "                    plt.title(f'{name} - Relationship between {sentiment_col} and {price_col}', fontsize=15)\n",
    "                    plt.grid(True)\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Calculate correlation\n",
    "                    # Ensure both series have the same index\n",
    "                    aligned_data = pd.merge(\n",
    "                        pd.DataFrame(time_series),\n",
    "                        pd.DataFrame(price_series),\n",
    "                        left_index=True, right_index=True,\n",
    "                        how='inner'\n",
    "                    )\n",
    "                    \n",
    "                    if not aligned_data.empty:\n",
    "                        correlation = aligned_data[sentiment_col].corr(aligned_data[price_col])\n",
    "                        print(f\"\\n6. Correlation coefficient between {sentiment_col} and {price_col}: {correlation:.4f}\")\n",
    "                        \n",
    "                        # Plot scatter plot\n",
    "                        plt.figure(figsize=(10, 6))\n",
    "                        plt.scatter(aligned_data[sentiment_col], aligned_data[price_col], alpha=0.6)\n",
    "                        plt.title(f'Scatter Plot of {sentiment_col} vs {price_col}', fontsize=15)\n",
    "                        plt.xlabel(sentiment_col)\n",
    "                        plt.ylabel(price_col)\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Time series analysis error: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze two datasets\n",
    "print(\"Starting cryptocurrency sentiment and price data exploratory analysis...\")\n",
    "\n",
    "# First analyze the final processed data\n",
    "try:\n",
    "    df_final = explore_dataset('df_Final.csv', 'Final Data')\n",
    "except Exception as e:\n",
    "    print(f\"Unable to load or analyze df_Final.csv: {e}\")\n",
    "\n",
    "# Then analyze the raw data\n",
    "try:\n",
    "    df_hourly = explore_dataset('Data_To_Hourervals_no_filter.csv', 'Hourly Data')\n",
    "except Exception as e:\n",
    "    print(f\"Unable to load or analyze Data_To_Hourervals_no_filter.csv: {e}\")\n",
    "\n",
    "print(\"\\nExploratory data analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_price_relationship(df, name):\n",
    "    \"\"\"Analyze the relationship between sentiment and price data\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"4. {name} - Sentiment and Price Relationship Analysis\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Find key columns\n",
    "    date_cols = [col for col in df.columns if any(term in col.lower() for term in ['date', 'time'])]\n",
    "    sentiment_cols = [col for col in df.columns if any(term in col.lower() for term in \n",
    "                     ['sentiment', 'compound', 'score', 'positive', 'negative'])]\n",
    "    price_cols = [col for col in df.columns if any(term in col.lower() for term in \n",
    "                 ['price', 'close', 'open', 'high', 'low'])]\n",
    "    \n",
    "    # Verify that necessary columns were found\n",
    "    if not (date_cols and sentiment_cols and price_cols):\n",
    "        print(\"Missing necessary columns (date, sentiment, or price). Please check the data structure.\")\n",
    "        missing = []\n",
    "        if not date_cols: missing.append(\"Date\")\n",
    "        if not sentiment_cols: missing.append(\"Sentiment\")  \n",
    "        if not price_cols: missing.append(\"Price\")\n",
    "        print(f\"Missing column types: {', '.join(missing)}\")\n",
    "        print(\"Available columns: \", df.columns.tolist())\n",
    "        return\n",
    "    \n",
    "    # Select main columns to use\n",
    "    date_col = date_cols[0]\n",
    "    sentiment_col = [col for col in sentiment_cols if 'compound' in col.lower() or 'score' in col.lower()]\n",
    "    sentiment_col = sentiment_col[0] if sentiment_col else sentiment_cols[0]\n",
    "    price_col = [col for col in price_cols if 'close' in col.lower()]\n",
    "    price_col = price_col[0] if price_col else price_cols[0]\n",
    "    \n",
    "    print(f\"Using the following columns for analysis:\")\n",
    "    print(f\"  - Date column: {date_col}\")\n",
    "    print(f\"  - Sentiment column: {sentiment_col}\")\n",
    "    print(f\"  - Price column: {price_col}\")\n",
    "    \n",
    "    # Ensure date column is datetime type\n",
    "    if df[date_col].dtype != 'datetime64[ns]':\n",
    "        print(f\"Converting date column '{date_col}' to datetime type...\")\n",
    "        try:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(f\"Date conversion failed: {e}\")\n",
    "            return\n",
    "    \n",
    "    # Check data quality\n",
    "    print(\"\\nData quality check:\")\n",
    "    print(f\"  - Date column missing values: {df[date_col].isnull().sum()} ({df[date_col].isnull().mean():.2%})\")\n",
    "    print(f\"  - Sentiment column missing values: {df[sentiment_col].isnull().sum()} ({df[sentiment_col].isnull().mean():.2%})\")\n",
    "    print(f\"  - Price column missing values: {df[price_col].isnull().sum()} ({df[price_col].isnull().mean():.2%})\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean = df.dropna(subset=[date_col, sentiment_col, price_col])\n",
    "    print(f\"After removing missing values, dataset rows reduced from {df.shape[0]} to {df_clean.shape[0]} ({(1 - df_clean.shape[0]/df.shape[0]):.2%} reduction)\")\n",
    "    \n",
    "    # Data normalization processing\n",
    "    df_analysis = df_clean.copy()\n",
    "    if df_analysis[date_col].dt.hour.nunique() > 1:\n",
    "        print(\"Detected hourly data, performing date aggregation...\")\n",
    "        df_analysis['date_day'] = df_analysis[date_col].dt.date\n",
    "        daily_sentiment = df_analysis.groupby('date_day')[sentiment_col].mean()\n",
    "        daily_price = df_analysis.groupby('date_day')[price_col].mean()\n",
    "        \n",
    "        # Merge daily data\n",
    "        daily_data = pd.DataFrame({\n",
    "            'date': daily_sentiment.index,\n",
    "            'sentiment': daily_sentiment.values,\n",
    "            'price': daily_price.values\n",
    "        })\n",
    "        daily_data.set_index('date', inplace=True)\n",
    "    else:\n",
    "        print(\"Using original date granularity...\")\n",
    "        daily_data = pd.DataFrame({\n",
    "            'sentiment': df_analysis[sentiment_col],\n",
    "            'price': df_analysis[price_col]\n",
    "        }, index=df_analysis[date_col])\n",
    "    \n",
    "    print(f\"Final analysis dataset shape: {daily_data.shape}\")\n",
    "    print(\"First 5 rows of data:\")\n",
    "    print(daily_data.head())\n",
    "    \n",
    "    # Plot time series trends\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Create dual Y-axis chart\n",
    "    fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "    \n",
    "    # Plot sentiment data\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Date', fontsize=12)\n",
    "    ax1.set_ylabel('Sentiment Score', color=color, fontsize=12)\n",
    "    ax1.plot(daily_data.index, daily_data['sentiment'], color=color, label='Sentiment Score')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Plot price data\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Price', color=color, fontsize=12)\n",
    "    ax2.plot(daily_data.index, daily_data['price'], color=color, label='Price')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Add grid and title\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    plt.title(f'{name} - Time Series Relationship between Sentiment Score and Price', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    correlation = daily_data['sentiment'].corr(daily_data['price'])\n",
    "    print(f\"\\nPearson correlation coefficient between sentiment and price: {correlation:.4f}\")\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(x='sentiment', y='price', data=daily_data, scatter_kws={'alpha':0.5})\n",
    "    plt.title(f'Scatter Plot of Sentiment Score vs Price (Correlation: {correlation:.4f})', fontsize=14)\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Lag analysis - whether sentiment leads or lags price\n",
    "    print(\"\\nLag correlation analysis:\")\n",
    "    max_lag = 7  # Analyze up to 7 days of lag\n",
    "    lag_results = {}\n",
    "    \n",
    "    # Correlation with sentiment leading price (positive lag)\n",
    "    print(\"Correlation with sentiment leading price:\")\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        lag_sentiment = daily_data['sentiment'].shift(lag)\n",
    "        lag_corr = lag_sentiment.corr(daily_data['price'])\n",
    "        lag_results[lag] = lag_corr\n",
    "        print(f\"  - Sentiment leading by {lag} days: {lag_corr:.4f}\")\n",
    "    \n",
    "    # Correlation with price leading sentiment (negative lag)\n",
    "    print(\"\\nCorrelation with price leading sentiment:\")\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        lag_price = daily_data['price'].shift(lag)\n",
    "        lag_corr = daily_data['sentiment'].corr(lag_price)\n",
    "        lag_results[-lag] = lag_corr\n",
    "        print(f\"  - Price leading by {lag} days: {lag_corr:.4f}\")\n",
    "    \n",
    "    # Visualize lag correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    lags = sorted(lag_results.keys())\n",
    "    corrs = [lag_results[lag] for lag in lags]\n",
    "    plt.bar(lags, corrs)\n",
    "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "    plt.title('Lag Correlation Analysis between Sentiment and Price', fontsize=14)\n",
    "    plt.xlabel('Lag Days (negative values indicate price leading sentiment, positive values indicate sentiment leading price)')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find the lag with maximum correlation\n",
    "    max_lag_key = max(lag_results, key=lambda k: abs(lag_results[k]))\n",
    "    print(f\"\\nMaximum correlation occurs at lag: {max_lag_key} days, correlation coefficient: {lag_results[max_lag_key]:.4f}\")\n",
    "    \n",
    "    if max_lag_key > 0:\n",
    "        print(f\"This suggests that sentiment changes may lead price changes by {max_lag_key} days\")\n",
    "    elif max_lag_key < 0:\n",
    "        print(f\"This suggests that price changes may lead sentiment changes by {abs(max_lag_key)} days\")\n",
    "    \n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relationship between price volatility and sentiment volatility\n",
    "try:\n",
    "    if 'merged_data' in locals():\n",
    "        # Calculate sentiment change and volatility\n",
    "        merged_data['sentiment_change'] = merged_data['sentiment'].diff()\n",
    "        merged_data['sentiment_volatility'] = merged_data['sentiment'].rolling(window=7).std()\n",
    "        \n",
    "        # Calculate price change percentage\n",
    "        merged_data['price_change_pct'] = merged_data['Adj Close'].pct_change() * 100\n",
    "        \n",
    "        # Plot relationship between sentiment volatility and price volatility\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.scatter(merged_data['sentiment_volatility'].dropna(), \n",
    "                   merged_data['Volatility'].dropna(), \n",
    "                   alpha=0.5)\n",
    "        plt.title('Relationship Between Sentiment Volatility and Price Volatility', fontsize=15)\n",
    "        plt.xlabel('Sentiment 7-Day Volatility')\n",
    "        plt.ylabel('Price 20-Day Volatility')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Check relationship between extreme sentiment and price changes\n",
    "        threshold = merged_data['sentiment'].std() * 1.5\n",
    "        merged_data['extreme_sentiment'] = np.where(\n",
    "            abs(merged_data['sentiment'] - merged_data['sentiment'].mean()) > threshold, \n",
    "            1, 0)\n",
    "        \n",
    "        # Compare price changes on extreme sentiment days vs normal sentiment days\n",
    "        extreme_days = merged_data[merged_data['extreme_sentiment'] == 1]['price_change_pct']\n",
    "        normal_days = merged_data[merged_data['extreme_sentiment'] == 0]['price_change_pct']\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.boxplot([normal_days.dropna(), extreme_days.dropna()], \n",
    "                   labels=['Normal Sentiment Days', 'Extreme Sentiment Days'])\n",
    "        plt.title('Comparison of Price Changes: Normal vs. Extreme Sentiment Days', fontsize=15)\n",
    "        plt.ylabel('Daily Price Change (%)')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Descriptive statistics\n",
    "        print(\"Price change statistics for normal sentiment days:\")\n",
    "        print(normal_days.describe())\n",
    "        \n",
    "        print(\"\\nPrice change statistics for extreme sentiment days:\")\n",
    "        print(extreme_days.describe())\n",
    "except Exception as e:\n",
    "    print(f\"Unable to perform further analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_analysis(df):\n",
    "    \"\"\"Main analysis workflow, integrating parts 4 and 5 of the analysis\"\"\"\n",
    "    # If data was loaded successfully, perform analysis\n",
    "    if not df.empty:\n",
    "        try:\n",
    "            # Execute part 4 - Sentiment and price relationship analysis\n",
    "            daily_data = analyze_sentiment_price_relationship(df, 'Cryptocurrency')\n",
    "            \n",
    "            # Execute part 5 - Advanced analysis\n",
    "            if daily_data is not None and not daily_data.empty:\n",
    "                result_df = advanced_exploratory_analysis(daily_data, 'Cryptocurrency')\n",
    "                \n",
    "                # Save processed data for subsequent modeling\n",
    "                if result_df is not None:\n",
    "                    result_df.to_csv('crypto_sentiment_analysis_data.csv')\n",
    "                    print(f\"Analysis completed, enhanced feature dataset generated\")\n",
    "                    print(f\"Final dataset shape: {result_df.shape}\")\n",
    "                return result_df\n",
    "            else:\n",
    "                print(\"Sentiment and price relationship analysis did not generate valid data\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during analysis: {e}\")\n",
    "    else:\n",
    "        print(\"Input data is empty, cannot perform analysis\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "# Main program\n",
    "print(\"Starting cryptocurrency sentiment and price data exploratory analysis...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# Load data\n",
    "    try:\n",
    "        df_final = pd.read_csv('df_Final.csv', sep=';', encoding='utf-8')\n",
    "        print(f\"Successfully loaded data, shape: {df_final.shape}\")\n",
    "        \n",
    "        # Execute core analysis\n",
    "        result_df = main_analysis(df_final)\n",
    "        \n",
    "        if result_df is not None:\n",
    "            print(\"Analysis completed, enhanced feature dataset generated\")\n",
    "            print(f\"Final dataset shape: {result_df.shape}\")\n",
    "        else:\n",
    "            print(\"Encountered issues during analysis, unable to generate final dataset\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or analyzing data: {e}\")\n",
    "\n",
    "    print(\"\\nExploratory data analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis enhancement example code\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize VADER analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "crypto_lexicon = {\n",
    "    \"moon\": 4.0,\n",
    "    \"hodl\": 2.0,\n",
    "    \"dump\": -3.0,\n",
    "    \"fud\": -3.0,\n",
    "    \"bullish\": 3.0,\n",
    "    \"bearish\": -3.0,\n",
    "    # Add more cryptocurrency-specific vocabulary\n",
    "}\n",
    "sia.lexicon.update(crypto_lexicon)\n",
    "\n",
    "# Apply enhanced sentiment analysis (if original text is available)\n",
    "if 'tweets_text' in df_final.columns:\n",
    "    # VADER enhanced analysis\n",
    "    df_final['vader_enhanced'] = df_final['tweets_text'].apply(\n",
    "        lambda x: sia.polarity_scores(x)['compound'] if isinstance(x, str) else None\n",
    "    )\n",
    "    \n",
    "    # TextBlob provides another sentiment measure\n",
    "    df_final['textblob_polarity'] = df_final['tweets_text'].apply(\n",
    "        lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else None\n",
    "    )\n",
    "    \n",
    "    # Sentiment intensity (regardless of positive/negative, only focusing on intensity)\n",
    "    df_final['sentiment_intensity'] = df_final['vader_enhanced'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified feature engineering and data preparation code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Load data\n",
    "try:\n",
    "    df_final = pd.read_csv('df_Final.csv', sep=';', encoding='utf-8')\n",
    "    print(f\"Successfully loaded data, original shape: {df_final.shape}\")\n",
    "    \n",
    "    # 2. Check data types\n",
    "    print(\"\\nData type check:\")\n",
    "    print(df_final.dtypes)\n",
    "    \n",
    "    # 3. Convert date column to datetime type\n",
    "    df_final['Date'] = pd.to_datetime(df_final['Date'], errors='coerce')\n",
    "    \n",
    "    # 4. Check non-numeric columns and process appropriately\n",
    "    object_columns = df_final.select_dtypes(include=['object']).columns.tolist()\n",
    "    if len(object_columns) > 0:\n",
    "        print(f\"\\nFound object type columns: {object_columns}\")\n",
    "        # Remove date column (if it appears in object columns)\n",
    "        if 'Date' in object_columns:\n",
    "            object_columns.remove('Date')\n",
    "        \n",
    "        # Try to convert remaining object columns to numeric\n",
    "        for col in object_columns:\n",
    "            try:\n",
    "                df_final[col] = pd.to_numeric(df_final[col], errors='coerce')\n",
    "                print(f\"  - Successfully converted column '{col}' to numeric type\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Unable to convert column '{col}' to numeric type: {e}\")\n",
    "    \n",
    "    # 5. Group by date and calculate daily averages (using only numeric columns)\n",
    "    print(\"\\nCreating daily summary data...\")\n",
    "    # Identify numeric columns\n",
    "    numeric_columns = df_final.select_dtypes(include=['number']).columns.tolist()\n",
    "    print(f\"Numeric columns ({len(numeric_columns)}): {numeric_columns[:5]}...\")  # Only show first 5\n",
    "    \n",
    "    # Group by date and calculate means (using only numeric columns)\n",
    "    df_daily = df_final.groupby(df_final['Date'].dt.date)[numeric_columns].mean().reset_index()\n",
    "    print(f\"Daily summary data shape: {df_daily.shape}\")\n",
    "    \n",
    "    # 6. Ensure date column is datetime type\n",
    "    df_daily['Date'] = pd.to_datetime(df_daily['Date'])\n",
    "    \n",
    "    # 7. Create features\n",
    "    print(\"\\nCreating features...\")\n",
    "    \n",
    "    # 7.1 Lag features - based on finding that price leads sentiment\n",
    "    lag_columns = []  # Track created lag columns\n",
    "    for i in range(1, 10):\n",
    "        # Find price columns (could be 'Open', 'Close', 'Price', etc.)\n",
    "        price_cols = [col for col in numeric_columns if any(term in col.lower() for term in ['open', 'close', 'price'])]\n",
    "        if price_cols:\n",
    "            price_col = price_cols[0]  # Use the first matching price column\n",
    "            col_name = f'price_lag_{i}'\n",
    "            df_daily[col_name] = df_daily[price_col].shift(i)\n",
    "            lag_columns.append(col_name)\n",
    "            \n",
    "            # Price change rate\n",
    "            col_name = f'price_change_{i}'\n",
    "            df_daily[col_name] = df_daily[price_col].pct_change(i) * 100\n",
    "            lag_columns.append(col_name)\n",
    "        \n",
    "        # Find sentiment columns\n",
    "        sentiment_cols = [col for col in numeric_columns if any(term in col.lower() for term in ['sentiment', 'compound'])]\n",
    "        if sentiment_cols:\n",
    "            sentiment_col = sentiment_cols[0]  # Use the first matching sentiment column\n",
    "            col_name = f'sentiment_lag_{i}'\n",
    "            df_daily[col_name] = df_daily[sentiment_col].shift(i)\n",
    "            lag_columns.append(col_name)\n",
    "    \n",
    "    # 7.2 Volatility features\n",
    "    windows = [3, 7, 14]  # Use shorter windows to reduce NaN\n",
    "    volatility_columns = []\n",
    "    for window in windows:\n",
    "        # Price volatility\n",
    "        if price_cols:\n",
    "            col_name = f'price_volatility_{window}d'\n",
    "            df_daily[col_name] = df_daily[price_col].pct_change().rolling(window).std()\n",
    "            volatility_columns.append(col_name)\n",
    "        \n",
    "        # Sentiment volatility\n",
    "        if sentiment_cols:\n",
    "            col_name = f'sentiment_volatility_{window}d'\n",
    "            df_daily[col_name] = df_daily[sentiment_col].rolling(window).std()\n",
    "            volatility_columns.append(col_name)\n",
    "    \n",
    "    # 7.3 Moving average features\n",
    "    ma_columns = []\n",
    "    for window in windows:\n",
    "        # Price moving average\n",
    "        if price_cols:\n",
    "            col_name = f'price_ma_{window}d'\n",
    "            df_daily[col_name] = df_daily[price_col].rolling(window).mean()\n",
    "            ma_columns.append(col_name)\n",
    "        \n",
    "        # Sentiment moving average\n",
    "        if sentiment_cols:\n",
    "            col_name = f'sentiment_ma_{window}d'\n",
    "            df_daily[col_name] = df_daily[sentiment_col].rolling(window).mean()\n",
    "            ma_columns.append(col_name)\n",
    "    \n",
    "    # 7.4 Trend features\n",
    "    if price_cols and 'price_ma_14d' in df_daily.columns:  # Use 14-day moving average to define trend\n",
    "        df_daily['trend'] = 'sideways'\n",
    "        df_daily.loc[df_daily[price_col] > df_daily['price_ma_14d'] * 1.05, 'trend'] = 'uptrend'\n",
    "        df_daily.loc[df_daily[price_col] < df_daily['price_ma_14d'] * 0.95, 'trend'] = 'downtrend'\n",
    "        \n",
    "        # Convert trend to numeric feature\n",
    "        trend_map = {'uptrend': 1, 'sideways': 0, 'downtrend': -1}\n",
    "        df_daily['trend_numeric'] = df_daily['trend'].map(trend_map)\n",
    "    \n",
    "    # 7.5 Extreme sentiment indicator\n",
    "    if sentiment_cols:\n",
    "        sentiment_mean = df_daily[sentiment_col].mean()\n",
    "        sentiment_std = df_daily[sentiment_col].std()\n",
    "        df_daily['extreme_sentiment'] = 0  # Normal sentiment\n",
    "        df_daily.loc[df_daily[sentiment_col] > sentiment_mean + 1.5 * sentiment_std, 'extreme_sentiment'] = 1  # Extreme positive\n",
    "        df_daily.loc[df_daily[sentiment_col] < sentiment_mean - 1.5 * sentiment_std, 'extreme_sentiment'] = -1  # Extreme negative\n",
    "    \n",
    "    # 7.6 Cross features\n",
    "    if price_cols and sentiment_cols:\n",
    "        df_daily['price_sentiment_ratio'] = df_daily[price_col] / (df_daily[sentiment_col].abs() + 0.01)  # Add 0.01 to avoid division by zero\n",
    "        \n",
    "        if 'price_change_1' in df_daily.columns:\n",
    "            df_daily['price_change_sentiment_product'] = df_daily['price_change_1'] * df_daily[sentiment_col]\n",
    "    \n",
    "    # 8. Remove NaN and save feature dataset\n",
    "    print(\"\\nRemoving missing values and saving dataset...\")\n",
    "    # Use forward fill to handle NaN (suitable for time series)\n",
    "    df_daily_filled = df_daily.fillna(method='ffill')\n",
    "    # Then use backward fill to handle NaN at the beginning\n",
    "    df_daily_filled = df_daily_filled.fillna(method='bfill')\n",
    "    # Finally use 0 for any remaining NaN\n",
    "    df_features = df_daily_filled.fillna(0)\n",
    "    \n",
    "    # Save processed dataset\n",
    "    df_features.to_csv('crypto_features_clean.csv', index=False)\n",
    "    print(f\"Feature engineering completed, generated {df_features.shape[1]} features, dataset shape: {df_features.shape}\")\n",
    "    print(f\"Data saved to 'crypto_features_clean.csv'\")\n",
    "    \n",
    "    # Output feature list for reference\n",
    "    print(\"\\nCreated feature list:\")\n",
    "    print(f\"- Lag features: {len(lag_columns)} columns\")\n",
    "    print(f\"- Volatility features: {len(volatility_columns)} columns\")\n",
    "    print(f\"- Moving average features: {len(ma_columns)} columns\")\n",
    "    print(\"- Trend features: 'trend', 'trend_numeric'\")\n",
    "    print(\"- Sentiment features: 'extreme_sentiment'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error encountered during processing: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation code\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Task 1: Predict future sentiment\n",
    "X_sentiment = df_features.drop(['Compound_Score', 'Date', 'trend'], axis=1)\n",
    "y_sentiment = df_features['Compound_Score']\n",
    "\n",
    "# Task 2: Predict future price\n",
    "# We try to predict price change 1 day in the future\n",
    "df_features['future_price_change_1d'] = df_features['Open'].pct_change(-1)  # -1 means 1 day in the future\n",
    "X_price = df_features.drop(['future_price_change_1d', 'Date', 'trend'], axis=1)\n",
    "y_price = df_features['future_price_change_1d']\n",
    "\n",
    "# Remove NaN\n",
    "X_sentiment = X_sentiment.fillna(0)\n",
    "y_sentiment = y_sentiment.fillna(0)\n",
    "X_price = X_price.fillna(0)\n",
    "y_price = y_price.fillna(0)\n",
    "\n",
    "# Data standardization\n",
    "scaler_X_sentiment = StandardScaler()\n",
    "X_sentiment_scaled = scaler_X_sentiment.fit_transform(X_sentiment)\n",
    "\n",
    "scaler_X_price = StandardScaler()\n",
    "X_price_scaled = scaler_X_price.fit_transform(X_price)\n",
    "\n",
    "# Data splitting - using time series split\n",
    "# For time series, it's better to split by time order rather than randomly\n",
    "train_size = int(len(X_sentiment_scaled) * 0.8)\n",
    "\n",
    "# Sentiment prediction task\n",
    "X_train_sentiment = X_sentiment_scaled[:train_size]\n",
    "X_test_sentiment = X_sentiment_scaled[train_size:]\n",
    "y_train_sentiment = y_sentiment[:train_size]\n",
    "y_test_sentiment = y_sentiment[train_size:]\n",
    "\n",
    "# Price prediction task\n",
    "X_train_price = X_price_scaled[:train_size]\n",
    "X_test_price = X_price_scaled[train_size:]\n",
    "y_train_price = y_price[:train_size]\n",
    "y_test_price = y_price[train_size:]\n",
    "\n",
    "print(\"Data preparation completed.\")\n",
    "print(f\"Training set size: {len(X_train_sentiment)}, Test set size: {len(X_test_sentiment)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
